model_name: lstm # regression, mlp, ngram or lstm
# add options as needed for dataset
run_name: "final_lstm4_uni"
training:
  lr: 0.001
  restore_ckpt: null
  num_epochs: 15
  loss_fn: "crossentropy"
  batch_size: 32
  optimizer: "adamw"
test:
  ckpt: null
logging: # frequency in number of iterations
  log_freq: 50
  val_freq: 500
  save_freq: 500
  ckpt_dir: "./ckpts"
data:
  use_start_end: False # to encode start, middle and end for each emotion (i.e. segmentation)
  train_filepath: "./data/new_final/final_data.json" # data file for training
  batch_first: True # batch first is default
  pack_seq: True # only pack for rnn/lstm
  bert_dim: 768 # dimension of encoding from bert
inference:
  output_file: "./test_results/predictions_lstm4_uni_4000.json"
  gt_json: null # name of gt json file
  checkpoint: "./ckpts/lstm/final_lstm4_uni/iter4000.pt" # name of checkpoint to load for model
  ngram: null # 1, 2 or 3
model: # Any model parameters
  hidden_dim: 256
  num_layers: 4
  bidirectional: False